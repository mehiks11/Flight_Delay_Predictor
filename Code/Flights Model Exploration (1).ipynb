{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "ecological-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "# munging imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve,f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-jenny",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-dimension",
   "metadata": {},
   "source": [
    "*In this notebook, I will use a smaller subset of the data to explore and compare the models where I need to dummify variables before choosing a final model and training on all the available data points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-buffalo",
   "metadata": {},
   "source": [
    "Pull in a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "acceptable-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in a 10,000 datapoint sample from our dataframe, with unecessary columns dropped.\n",
    "sample_df=pd.read_csv('/Users/mehikapatel/Flights_Project/Data/FinalFlightsData.csv').drop(columns=['Unnamed: 0',\n",
    "                                                                                                    'DEP_TIME',\n",
    "                                                                                                    'DEP_DELAY',\n",
    "                                                                                                    'ARR_DELAY',\n",
    "                                                                                                    'CANCELLED',\n",
    "                                                                                                    'DATE',\n",
    "                                                                                                    'FLIGHT_NUM',\n",
    "                                                                                                    'origin_lat',\n",
    "                                                                                                    'origin_lon',\n",
    "                                                                                                    'dest_lat',\n",
    "                                                                                                    'dest_lon',\n",
    "                                                                                                    'CRS_DEP_TIME',\n",
    "                                                                                                    'CRS_ARR_TIME',\n",
    "                                                                                                   'municipality1',\n",
    "                                                                                                   'municipality2',\n",
    "                                                                                                   'origin_type','dest_type']).sample(n=10000,\n",
    "                                                                                                                        random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "harmful-calibration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40000 entries, 6469840 to 1051986\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   AIRLINE           40000 non-null  object \n",
      " 1   ORIGIN            40000 non-null  object \n",
      " 2   DEST              40000 non-null  object \n",
      " 3   CRS_ELAPSED_TIME  40000 non-null  float64\n",
      " 4   DISTANCE          40000 non-null  float64\n",
      " 5   MONTH             40000 non-null  int64  \n",
      " 6   DAYOFWEEK         40000 non-null  int64  \n",
      " 7   holiday_szn       40000 non-null  bool   \n",
      " 8   DEP_HOUR          40000 non-null  int64  \n",
      " 9   ARR_HOUR          40000 non-null  int64  \n",
      " 10  target            40000 non-null  bool   \n",
      " 11  origin_weather    19654 non-null  object \n",
      " 12  origin_severity   19654 non-null  object \n",
      " 13  dest_weather      19656 non-null  object \n",
      " 14  dest_severity     19656 non-null  object \n",
      "dtypes: bool(2), float64(2), int64(4), object(7)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-consultation",
   "metadata": {},
   "source": [
    "Fill NA values for weather information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "collected-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.fillna({'origin_weather':'Normal','origin_severity':0,'dest_weather':'Normal','dest_severity':0},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-compact",
   "metadata": {},
   "source": [
    "Dummify all features to make them viable for model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "passing-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Airline\n",
    "sample_df = sample_df.join(pd.get_dummies(sample_df['AIRLINE'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('AIRLINE',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "classified-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Origin\n",
    "sample_df = sample_df.join(pd.get_dummies(sample_df['ORIGIN'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('ORIGIN',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oakland",
   "metadata": {},
   "source": [
    "Because we have matching values in the origin and dest columns and have already made dummy variables for all our origin airports, we must change those in the destination column slightly to be able to make dummy variables pertaining to the destination. So, we coerce the destination column by adding \"2\" to each data point before making dummies to indicate destination airport code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "fitted-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['DEST'] = sample_df['DEST'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "suitable-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dest\n",
    "sample_df = sample_df.join(pd.get_dummies(sample_df['DEST'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('DEST',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "southeast-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month\n",
    "    #two features cos and sin:\n",
    "sample_df['sin_month']=np.sin(2*np.pi*sample_df.MONTH/12)\n",
    "sample_df['cos_month']=np.cos(2*np.pi*sample_df.MONTH/12)\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('MONTH',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "trained-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day of Week\n",
    "    #two features cos and sin:\n",
    "sample_df['sin_DOW']=np.sin(2*np.pi*sample_df.DAYOFWEEK/7)\n",
    "sample_df['cos_DOW']=np.cos(2*np.pi*sample_df.DAYOFWEEK/7)\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('DAYOFWEEK',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "stainless-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #origin Type\n",
    "# sample_df = sample_df.join(pd.get_dummies(sample_df['origin_type'],drop_first=True))\n",
    "\n",
    "# #drop original\n",
    "# sample_df.drop('origin_type',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-traveler",
   "metadata": {},
   "source": [
    "Similar to above with our airport codes, we have to slightly change the destination type by adding \"2\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "civic-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df['dest_type'] = sample_df['dest_type'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "express-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dest type\n",
    "# sample_df = sample_df.join(pd.get_dummies(sample_df['dest_type'],drop_first=True))\n",
    "\n",
    "# #drop original\n",
    "# sample_df.drop('dest_type',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "individual-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dep hour\n",
    "sample_df['sin_DEP_HOUR']=np.sin(2*np.pi*sample_df.DEP_HOUR/24)\n",
    "sample_df['cos_DEP_HOUR']=np.cos(2*np.pi*sample_df.DEP_HOUR/24)\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('DEP_HOUR',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "toxic-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arr hour\n",
    "sample_df['sin_ARR_HOUR']=np.sin(2*np.pi*sample_df.ARR_HOUR/24)\n",
    "sample_df['cos_ARR_HOUR']=np.cos(2*np.pi*sample_df.ARR_HOUR/24)\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('ARR_HOUR',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "marked-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin weather\n",
    "sample_df = sample_df.join(pd.get_dummies(sample_df['origin_weather'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('origin_weather',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-summit",
   "metadata": {},
   "source": [
    "Again, we will do the same for destination weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "advance-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['dest_weather'] = sample_df['dest_weather'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "crazy-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest weather\n",
    "sample_df = sample_df.join(pd.get_dummies(sample_df['dest_weather'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "sample_df.drop('dest_weather',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-confirmation",
   "metadata": {},
   "source": [
    "We will be converting the weather severity columns into ordinal data.\n",
    "\n",
    "Specifically, there are 6 values:\n",
    "1. Light\n",
    "2. Moderate\n",
    "3. Severe\n",
    "4. UNK (unknown)\n",
    "5. Heavy\n",
    "6. Other\n",
    "\n",
    "For the light,moderate, heavy, and severe, we will make light = 1, moderate = 2, heavy = 3, severe =4. \n",
    "The other categories will be removed from the dataset since we do not definetively know, and this could skew our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "civilian-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unknown and others fully from dataset considering they have weather data but no severity level.\n",
    "sample_df = sample_df[sample_df.origin_severity != 'UNK']\n",
    "sample_df = sample_df[sample_df.origin_severity != 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "experimental-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin severity\n",
    "\n",
    "# lambda x: x*10 if x<2 else (x**2 if x<4 else x+10)\n",
    "\n",
    "sample_df['origin_sev'] =sample_df['origin_severity'].apply(lambda x: 1 if x == \"Light\" else(2 if x == \"Moderate\" else(3 if x == \"Heavy\" else (4 if x== \"Severe\" else 0 ))))\n",
    "sample_df.drop(columns=['origin_severity'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "architectural-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest severity\n",
    "sample_df['dest_sev'] =sample_df['dest_severity'].apply(lambda x: 1 if x == \"Light\" else(2 if x == \"Moderate\" else(3 if x == \"Heavy\" else (4 if x== \"Severe\" else 0 ))))\n",
    "\n",
    "#drop original columns for origin and dest severity\n",
    "sample_df.drop(columns=['dest_severity'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-things",
   "metadata": {},
   "source": [
    "## Metric Decision: F1\n",
    "\n",
    "The metric that will be used to compare models will be the balanced F1 metric to find an optimal balance between precision and recall. We imagine it is equally harmful/beneficial to increase our recall rate as it is to increase our precision rate when classifiying a future flight as \"delayed\" or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-georgia",
   "metadata": {},
   "source": [
    "## Dealing with Class Imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "typical-stevens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    81158\n",
       "True     17507\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "empty-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of NOT Delayed is 83%\n",
      "The percentage of Delayed is 17%\n"
     ]
    }
   ],
   "source": [
    "Fal = 8148\n",
    "Tru = 1715\n",
    "Tot = Fal + Tru\n",
    "\n",
    "print(f'The percentage of NOT Delayed is {round((Fal/Tot)*100)}%')\n",
    "print(f'The percentage of Delayed is {round((Tru/Tot)*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-bottom",
   "metadata": {},
   "source": [
    "Because our target classes are strongly imbalanced (83% vs 17%), we will use class_weights when modeling to make sure our models catch our smaller class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-wagner",
   "metadata": {},
   "source": [
    "## Create our first simple model with the sample data with all the features to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-company",
   "metadata": {},
   "source": [
    "Create basic KNN & Logistic Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "proprietary-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test sets:\n",
    "X = sample_df.drop(columns=['target'])\n",
    "y = sample_df.target\n",
    "\n",
    "#split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-canada",
   "metadata": {},
   "source": [
    "#### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "saved-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X_train_scaled = std_scale.fit_transform(X_train)\n",
    "X_test_scaled = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "naughty-ecology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.777289624873268\n",
      "Precision Score: 0.18421052631578946\n",
      "Recall Score: 0.08155339805825243\n",
      "F1 Score: 0.11305518169582773\n"
     ]
    }
   ],
   "source": [
    "# Train on training set, and Test on testing set\n",
    "knn = KNeighborsClassifier(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_test, y_pred)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_test, y_pred)}')\n",
    "\n",
    "# Accuracy Score: 0.7161088573360379\n",
    "# Precision Score: 0.1987724268177526\n",
    "# Recall Score: 0.20211233797407585\n",
    "# F1 Score: 0.20042846941204476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-chapter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neutral-headline",
   "metadata": {},
   "source": [
    "#### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "hairy-opera",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5542412977357215\n",
      "Precision Score: 0.21326676176890158\n",
      "Recall Score: 0.5805825242718446\n",
      "F1 Score: 0.3119457485654669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehikapatel/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight={0:1,1:5.3}) # setting C very high essentially removes regularization\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_test, y_pred)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_test, y_pred)}')\n",
    "\n",
    "# Accuracy Score: 0.8196416497633536\n",
    "# Precision Score: 0.2616822429906542\n",
    "# Recall Score: 0.01344215074411906\n",
    "# F1 Score: 0.02557077625570776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "subtle-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6423812282734647\n",
      "Precision Score: 0.2955204640670319\n",
      "Recall Score: 0.7641666666666667\n",
      "F1 Score: 0.4262142691145712\n"
     ]
    }
   ],
   "source": [
    "# training data metrics: Shows this is JUST a bad Model!\n",
    "\n",
    "y_pred = lr.predict(X_train_scaled)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_train, y_pred)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_train, y_pred)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_train, y_pred)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_train, y_pred)}')\n",
    "\n",
    "# Accuracy Score: 0.8248143452273139\n",
    "# Precision Score: 0.5666666666666667\n",
    "# Recall Score: 0.020987654320987655\n",
    "# F1 Score: 0.04047619047619047"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-trouble",
   "metadata": {},
   "source": [
    "### The following is a lump of code that removes columns with zeroed out coefficient in the logistic model. ( but on a copy of sample_df so we can readjust things as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "suffering-penguin",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-940-a74c58233b9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "exotic-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-904-3e048d994116>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zeroed_cos.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "colums = pd.Series(copy_df.columns)\n",
    "coefs = pd.Series(lr.coef_[0]).abs()\n",
    "frames = [colums,coefs]\n",
    "coeficients = pd.concat(frames,axis=1)\n",
    "coeficients.rename({0:'Col',1:'Coef'},axis=1,inplace=True)\n",
    "zeroed_cos = coeficients[coeficients['Coef']==0]\n",
    "zeroed_cos.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "rocky-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns with lr coef 0.\n",
    "copy_df.drop(columns=zeroed_cos.Col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "local-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9863 entries, 6469840 to 6616243\n",
      "Columns: 543 entries, CRS_ELAPSED_TIME to dest_sev\n",
      "dtypes: bool(2), float64(9), int64(2), uint8(530)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "#to see how many columns are left\n",
    "copy_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-article",
   "metadata": {},
   "source": [
    "Fit logistic on this copy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "proved-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test sets:\n",
    "Xcop = copy_df.drop(columns=['target'])\n",
    "ycop = copy_df.target\n",
    "\n",
    "#split into train and test\n",
    "X_traincop, X_testcop, y_traincop, y_testcop = train_test_split(Xcop, ycop, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "opened-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X_train_scaledcop = std_scale.fit_transform(X_traincop)\n",
    "X_test_scaledcop = std_scale.transform(X_testcop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "logical-cedar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5552551537681649\n",
      "Precision Score: 0.21855235418130708\n",
      "Recall Score: 0.6038834951456311\n",
      "F1 Score: 0.32094943240454077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight={0:1,1:5.3}) # setting C very high essentially removes regularization\n",
    "lr.fit(X_train_scaledcop, y_traincop)\n",
    "\n",
    "y_predcop = lr.predict(X_test_scaledcop)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_testcop, y_predcop)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_testcop, y_predcop)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_testcop, y_predcop)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_testcop, y_predcop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "enclosed-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5376816492058127\n",
      "Precision Score: 0.21736249171636846\n",
      "Recall Score: 0.6368932038834951\n",
      "F1 Score: 0.3241106719367589\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(class_weight={0:1,1:5.9}) # setting C very high essentially removes regularization\n",
    "lr2.fit(X_train_scaledcop, y_traincop)\n",
    "\n",
    "\n",
    "y_predcop = (lr2.predict_proba(X_test_scaledcop)[:,1] >= 0.51).astype(bool)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_testcop, y_predcop)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_testcop, y_predcop)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_testcop, y_predcop)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_testcop, y_predcop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-circumstances",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
