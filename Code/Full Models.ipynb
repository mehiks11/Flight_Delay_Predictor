{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecological-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "# munging imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve,f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-jenny",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-dimension",
   "metadata": {},
   "source": [
    "*In this notebook, I will use a smaller subset of the data to explore and compare the models where I need to dummify variables before choosing a final model and training on all the available data points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-buffalo",
   "metadata": {},
   "source": [
    "Pull in a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acceptable-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in all data\n",
    "df=pd.read_csv('/Users/mehikapatel/Flights_Project/Data/FinalFlightsData.csv').drop(columns=['Unnamed: 0',\n",
    "                                                                                                    'DEP_TIME',\n",
    "                                                                                                    'DEP_DELAY',\n",
    "                                                                                                    'ARR_DELAY',\n",
    "                                                                                                    'CANCELLED',\n",
    "                                                                                                    'DATE',\n",
    "                                                                                                    'FLIGHT_NUM',\n",
    "                                                                                                    'origin_lat',\n",
    "                                                                                                    'origin_lon',\n",
    "                                                                                                    'dest_lat',\n",
    "                                                                                                    'dest_lon',\n",
    "                                                                                                    'CRS_DEP_TIME',\n",
    "                                                                                                    'CRS_ARR_TIME',\n",
    "                                                                                                   'municipality1',\n",
    "                                                                                                   'municipality2',\n",
    "                                                                                                   'origin_type','dest_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-murray",
   "metadata": {},
   "source": [
    "Fill NA values for weather information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({'origin_weather':'Normal','origin_severity':0,'dest_weather':'Normal','dest_severity':0},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-compact",
   "metadata": {},
   "source": [
    "Dummify all features to make them viable for model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passing-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Airline\n",
    "df = df.join(pd.get_dummies(df['AIRLINE'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "df.drop('AIRLINE',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "classified-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Origin\n",
    "df = df.join(pd.get_dummies(df['ORIGIN'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "df.drop('ORIGIN',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oakland",
   "metadata": {},
   "source": [
    "Because we have matching values in the origin and dest columns and have already made dummy variables for all our origin airports, we must change those in the destination column slightly to be able to make dummy variables pertaining to the destination. So, we coerce the destination column by adding \"2\" to each data point before making dummies to indicate destination airport code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitted-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEST'] = df['DEST'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suitable-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dest\n",
    "df = df.join(pd.get_dummies(df['DEST'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "df.drop('DEST',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southeast-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month\n",
    "    #two features cos and sin:\n",
    "df['sin_month']=np.sin(2*np.pi*df.MONTH/12)\n",
    "df['cos_month']=np.cos(2*np.pi*df.MONTH/12)\n",
    "\n",
    "#drop original\n",
    "df.drop('MONTH',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "trained-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day of Week\n",
    "    #two features cos and sin:\n",
    "df['sin_DOW']=np.sin(2*np.pi*df.DAYOFWEEK/7)\n",
    "df['cos_DOW']=np.cos(2*np.pi*df.DAYOFWEEK/7)\n",
    "\n",
    "#drop original\n",
    "df.drop('DAYOFWEEK',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stainless-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #origin Type\n",
    "# sample_df = sample_df.join(pd.get_dummies(sample_df['origin_type'],drop_first=True))\n",
    "\n",
    "# #drop original\n",
    "# sample_df.drop('origin_type',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-traveler",
   "metadata": {},
   "source": [
    "Similar to above with our airport codes, we have to slightly change the destination type by adding \"2\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civic-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df['dest_type'] = sample_df['dest_type'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "express-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dest type\n",
    "# sample_df = sample_df.join(pd.get_dummies(sample_df['dest_type'],drop_first=True))\n",
    "\n",
    "# #drop original\n",
    "# sample_df.drop('dest_type',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "individual-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dep hour\n",
    "df['sin_DEP_HOUR']=np.sin(2*np.pi*df.DEP_HOUR/24)\n",
    "df['cos_DEP_HOUR']=np.cos(2*np.pi*df.DEP_HOUR/24)\n",
    "\n",
    "#drop original\n",
    "df.drop('DEP_HOUR',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "toxic-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arr hour\n",
    "df['sin_ARR_HOUR']=np.sin(2*np.pi*df.ARR_HOUR/24)\n",
    "df['cos_ARR_HOUR']=np.cos(2*np.pi*df.ARR_HOUR/24)\n",
    "\n",
    "#drop original\n",
    "df.drop('ARR_HOUR',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "marked-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin weather\n",
    "df = df.join(pd.get_dummies(df['origin_weather'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "df.drop('origin_weather',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-summit",
   "metadata": {},
   "source": [
    "Again, we will do the same for destination weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "advance-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dest_weather'] = df['dest_weather'].astype(str) + '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "crazy-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest weather\n",
    "df = df.join(pd.get_dummies(df['dest_weather'],drop_first=True))\n",
    "\n",
    "#drop original\n",
    "df.drop('dest_weather',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-confirmation",
   "metadata": {},
   "source": [
    "We will be converting the weather severity columns into ordinal data.\n",
    "\n",
    "Specifically, there are 6 values:\n",
    "1. Light\n",
    "2. Moderate\n",
    "3. Severe\n",
    "4. UNK (unknown)\n",
    "5. Heavy\n",
    "6. Other\n",
    "\n",
    "For the light,moderate, heavy, and severe, we will make light = 1, moderate = 2, heavy = 3, severe =4. \n",
    "The other categories will be removed from the dataset since we do not definetively know, and this could skew our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "civilian-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unknown and others fully from dataset considering they have weather data but no severity level.\n",
    "df = df[df.origin_severity != 'UNK']\n",
    "df = df[df.origin_severity != 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "experimental-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin severity\n",
    "\n",
    "# lambda x: x*10 if x<2 else (x**2 if x<4 else x+10)\n",
    "\n",
    "df['origin_sev'] =df['origin_severity'].apply(lambda x: 1 if x == \"Light\" else(2 if x == \"Moderate\" else(3 if x == \"Heavy\" else (4 if x== \"Severe\" else 0 ))))\n",
    "df.drop(columns=['origin_severity'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "architectural-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest severity\n",
    "df['dest_sev'] = df['dest_severity'].apply(lambda x: 1 if x == \"Light\" else(2 if x == \"Moderate\" else(3 if x == \"Heavy\" else (4 if x== \"Severe\" else 0 ))))\n",
    "\n",
    "#drop original columns for origin and dest severity\n",
    "df.drop(columns=['dest_severity'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "posted-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/Users/mehikapatel/Flights_Project/FlightsDummified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-wagner",
   "metadata": {},
   "source": [
    "## Create our Final Full models with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "animated-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12306041 entries, 0 to 12477211\n",
      "Columns: 761 entries, CRS_ELAPSED_TIME to dest_sev\n",
      "dtypes: bool(2), float64(10), int64(2), uint8(747)\n",
      "memory usage: 9.8 GB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "breeding-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = df.sample(300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-company",
   "metadata": {},
   "source": [
    "Create basic KNN & Logistic Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "proprietary-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test sets:\n",
    "X = df_samp.drop(columns=['target'])\n",
    "y = df_samp.target\n",
    "\n",
    "#split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "visible-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X_train_scaled = std_scale.fit_transform(X_train)\n",
    "X_test_scaled = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-headline",
   "metadata": {},
   "source": [
    "#### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "hairy-opera",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehikapatel/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight={0:1,1:5.3}) # setting C very high essentially removes regularization\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "honey-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5437888888888889\n",
      "Precision Score: 0.23445269684081338\n",
      "Recall Score: 0.6942355889724311\n",
      "F1 Score: 0.35052753128015307\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy Score: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_test, y_pred)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sorted-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5476095238095238\n",
      "Precision Score: 0.23858359658571468\n",
      "Recall Score: 0.7077683198625171\n",
      "F1 Score: 0.3568691696340324\n"
     ]
    }
   ],
   "source": [
    "# training data metrics: Shows this is JUST a bad Model!\n",
    "\n",
    "y_pred = lr.predict(X_train_scaled)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_train, y_pred)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_train, y_pred)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_train, y_pred)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_train, y_pred)}')\n",
    "\n",
    "# Accuracy Score: 0.8248143452273139\n",
    "# Precision Score: 0.5666666666666667\n",
    "# Recall Score: 0.020987654320987655\n",
    "# F1 Score: 0.04047619047619047"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-trouble",
   "metadata": {},
   "source": [
    "### The following is a lump of code that removes columns with zeroed out coefficient in the logistic model. ( but on a copy of sample_df so we can readjust things as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "modular-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df_samp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "exotic-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-3e048d994116>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zeroed_cos.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "colums = pd.Series(copy_df.columns)\n",
    "coefs = pd.Series(lr.coef_[0]).abs()\n",
    "frames = [colums,coefs]\n",
    "coeficients = pd.concat(frames,axis=1)\n",
    "coeficients.rename({0:'Col',1:'Coef'},axis=1,inplace=True)\n",
    "zeroed_cos = coeficients[coeficients['Coef']==0]\n",
    "zeroed_cos.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "rocky-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns with lr coef 0.\n",
    "copy_df.drop(columns=zeroed_cos.Col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "local-dakota",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300000 entries, 2196431 to 7098487\n",
      "Columns: 761 entries, CRS_ELAPSED_TIME to dest_sev\n",
      "dtypes: bool(2), float64(10), int64(2), uint8(747)\n",
      "memory usage: 244.0 MB\n"
     ]
    }
   ],
   "source": [
    "#to see how many columns are left\n",
    "copy_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-witness",
   "metadata": {},
   "source": [
    "Fit logistic on this copy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "herbal-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test sets:\n",
    "Xcop = copy_df.drop(columns=['target'])\n",
    "ycop = copy_df.target\n",
    "\n",
    "#split into train and test\n",
    "X_traincop, X_testcop, y_traincop, y_testcop = train_test_split(Xcop, ycop, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "compliant-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X_train_scaledcop = std_scale.fit_transform(X_traincop)\n",
    "X_test_scaledcop = std_scale.transform(X_testcop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "coordinated-monthly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5422555555555556\n",
      "Precision Score: 0.2338599118385253\n",
      "Recall Score: 0.6947368421052632\n",
      "F1 Score: 0.34992820286241777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight={0:1,1:5.3}) # setting C very high essentially removes regularization\n",
    "lr.fit(X_train_scaledcop, y_traincop)\n",
    "\n",
    "y_predcop = lr.predict(X_test_scaledcop)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_testcop, y_predcop)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_testcop, y_predcop)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_testcop, y_predcop)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_testcop, y_predcop)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-exemption",
   "metadata": {},
   "source": [
    "* Accuracy Score: 0.5422555555555556\n",
    "* Precision Score: 0.2338599118385253\n",
    "* Recall Score: 0.6947368421052632\n",
    "* F1 Score: 0.34992820286241777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "allied-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5115888888888889\n",
      "Precision Score: 0.22774568722406985\n",
      "Recall Score: 0.7337092731829574\n",
      "F1 Score: 0.3475963607759325\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(class_weight={0:1,1:5.9}) # setting C very high essentially removes regularization\n",
    "lr2.fit(X_train_scaledcop, y_traincop)\n",
    "\n",
    "\n",
    "y_predcop = (lr2.predict_proba(X_test_scaledcop)[:,1] >= 0.51).astype(bool)\n",
    "\n",
    "print(f'Accuracy Score: {metrics.accuracy_score(y_testcop, y_predcop)}')\n",
    "print(f'Precision Score: {metrics.precision_score(y_testcop, y_predcop)}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_testcop, y_predcop)}')\n",
    "print(f'F1 Score: {metrics.f1_score(y_testcop, y_predcop)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-formation",
   "metadata": {},
   "source": [
    "2:\n",
    "* Accuracy Score: 0.5118\n",
    "* Precision Score: 0.22763911062653325\n",
    "* Recall Score: 0.7325814536340852\n",
    "* F1 Score: 0.3473455928225543\n",
    "\n",
    "3:\n",
    "* Accuracy Score: 0.5434888888888889\n",
    "* Precision Score: 0.23395874804116726\n",
    "* Recall Score: 0.6922305764411028\n",
    "* F1 Score: 0.34971985692127505"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-ready",
   "metadata": {},
   "source": [
    "Our above iterations do not improve the model, so we will stick with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "flying-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it on certain flights!\n",
    "\n",
    "test_points = df.sample(3)\n",
    "X = test_points.drop(columns=['target'])\n",
    "y = test_points.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "loaded-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X = std_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "efficient-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "korean-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11623755    False\n",
       "5811482     False\n",
       "9822348      True\n",
       "Name: target, dtype: bool"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "married-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRS_ELAPSED_TIME     80.0\n",
       "DISTANCE            337.0\n",
       "holiday_szn         False\n",
       "target              False\n",
       "Allegiant Air           0\n",
       "                    ...  \n",
       "Rain2                   0\n",
       "Snow2                   0\n",
       "Storm2                  0\n",
       "origin_sev              0\n",
       "dest_sev                2\n",
       "Name: 1514261, Length: 761, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[11623755]\n",
    "df.loc[5811482]\n",
    "df.loc[9822348]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-florist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
